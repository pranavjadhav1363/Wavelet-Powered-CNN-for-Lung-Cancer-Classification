{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRANAV JADHAV\\AppData\\Local\\Temp\\ipykernel_16568\\403246102.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PRANAV JADHAV\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../../DATA_PROCESSED/64x64/\"\n",
    "categories = [\"Benign\", \"Malignant\", \"Normal\"]\n",
    "image_size = (64, 64)\n",
    "\n",
    "# Load images and labels with filenames\n",
    "data = []\n",
    "labels = []\n",
    "file_names = []  # List to store image file names\n",
    "\n",
    "# %%\n",
    "for category in categories:\n",
    "    category_path = os.path.join(data_dir, category)\n",
    "    for img_name in os.listdir(category_path):\n",
    "        img_path = os.path.join(category_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, image_size)\n",
    "        data.append(img)\n",
    "        labels.append(categories.index(category))\n",
    "        file_names.append(img_name)  # Store the filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data).reshape(-1, 64, 64, 1).astype('float32') / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels, train_file_names, test_file_names = train_test_split(\n",
    "    data, labels, file_names, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "# %%\n",
    "train_labels = to_categorical(train_labels, num_classes=len(categories))\n",
    "test_labels = to_categorical(test_labels, num_classes=len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(\n",
    "        32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Flatten the output from the convolutional layers\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "    # Output layer (3 classes)\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PRANAV JADHAV\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\PRANAV JADHAV\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 31, 31, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               2359808   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2617091 (9.98 MB)\n",
      "Trainable params: 2617091 (9.98 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (64, 64, 1)  # Example input shape for grayscale images\n",
    "num_classes = len(categories)  # Number of classes\n",
    "cnn_model = create_model(input_shape, num_classes)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\PRANAV JADHAV\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\PRANAV JADHAV\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\PRANAV JADHAV\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "277/277 [==============================] - 31s 107ms/step - loss: 0.9413 - accuracy: 0.5624 - val_loss: 0.9254 - val_accuracy: 0.5676\n",
      "Epoch 2/10\n",
      "277/277 [==============================] - 31s 111ms/step - loss: 0.8705 - accuracy: 0.6003 - val_loss: 0.7177 - val_accuracy: 0.7033\n",
      "Epoch 3/10\n",
      "277/277 [==============================] - 32s 115ms/step - loss: 0.5807 - accuracy: 0.7716 - val_loss: 0.4531 - val_accuracy: 0.8164\n",
      "Epoch 4/10\n",
      "277/277 [==============================] - 40s 143ms/step - loss: 0.3620 - accuracy: 0.8511 - val_loss: 0.3039 - val_accuracy: 0.8797\n",
      "Epoch 5/10\n",
      "277/277 [==============================] - 44s 157ms/step - loss: 0.2312 - accuracy: 0.9076 - val_loss: 0.1782 - val_accuracy: 0.9267\n",
      "Epoch 6/10\n",
      "277/277 [==============================] - 30s 107ms/step - loss: 0.1264 - accuracy: 0.9552 - val_loss: 0.1274 - val_accuracy: 0.9543\n",
      "Epoch 7/10\n",
      "277/277 [==============================] - 32s 115ms/step - loss: 0.0911 - accuracy: 0.9675 - val_loss: 0.0904 - val_accuracy: 0.9670\n",
      "Epoch 8/10\n",
      "277/277 [==============================] - 29s 104ms/step - loss: 0.0366 - accuracy: 0.9877 - val_loss: 0.0680 - val_accuracy: 0.9769\n",
      "Epoch 9/10\n",
      "277/277 [==============================] - 30s 109ms/step - loss: 0.0407 - accuracy: 0.9868 - val_loss: 0.0783 - val_accuracy: 0.9769\n",
      "Epoch 10/10\n",
      "277/277 [==============================] - 30s 107ms/step - loss: 0.0220 - accuracy: 0.9928 - val_loss: 0.0357 - val_accuracy: 0.9873\n"
     ]
    }
   ],
   "source": [
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = cnn_model.fit(train_data, train_labels, epochs=10,\n",
    "                        batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 3s 32ms/step\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Recall: 0.99\n",
      "ROC-AUC: 1.00\n",
      "Specificity: [0.97096774 0.99299363 0.98414496]\n",
      "Confusion Matrix:\n",
      "[[ 301    0    7]\n",
      " [   2 1559    7]\n",
      " [   7   11  869]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign       0.97      0.98      0.97       308\n",
      "   Malignant       0.99      0.99      0.99      1568\n",
      "      Normal       0.98      0.98      0.98       887\n",
      "\n",
      "    accuracy                           0.99      2763\n",
      "   macro avg       0.98      0.98      0.98      2763\n",
      "weighted avg       0.99      0.99      0.99      2763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "predictions = cnn_model.predict(test_data)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_classes, predicted_classes)\n",
    "precision = precision_score(\n",
    "    true_classes, predicted_classes, average='weighted')\n",
    "recall = recall_score(true_classes, predicted_classes, average='weighted')\n",
    "roc_auc = roc_auc_score(test_labels, predictions, multi_class='ovr')\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Calculate specificity for each class\n",
    "tn = conf_matrix.diagonal()\n",
    "fp = conf_matrix.sum(axis=0) - tn\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# %%\n",
    "report = classification_report(\n",
    "    true_classes, predicted_classes, target_names=categories)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'ROC-AUC: {roc_auc:.2f}')\n",
    "print(f'Specificity: {specificity}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "# Create DataFrames for training and testing data\n",
    "train_data_df = pd.DataFrame(\n",
    "    {'File Name': train_file_names, 'Class': np.argmax(train_labels, axis=1)})\n",
    "test_data_df = pd.DataFrame(\n",
    "    {'File Name': test_file_names, 'Class': np.argmax(test_labels, axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('image_data.xlsx') as writer:\n",
    "    train_data_df.to_excel(writer, sheet_name='Train Data', index=False)\n",
    "    test_data_df.to_excel(writer, sheet_name='Test Data', index=False)\n",
    "\n",
    "    # Store metrics in a separate sheet\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Metric': ['Accuracy', 'Precision', 'Recall', 'ROC-AUC'],\n",
    "        'Value': [accuracy, precision, recall, roc_auc]\n",
    "    })\n",
    "    metrics_df.to_excel(writer, sheet_name='Metrics', index=False)\n",
    "\n",
    "    # Store confusion matrix in a separate sheet\n",
    "    confusion_df = pd.DataFrame(\n",
    "        conf_matrix, index=categories, columns=categories)\n",
    "    confusion_df.to_excel(writer, sheet_name='Confusion Matrix')\n",
    "\n",
    "    # Store specificity in a separate sheet\n",
    "    specificity_df = pd.DataFrame({\n",
    "        'Class': categories,\n",
    "        'Specificity': specificity\n",
    "    })\n",
    "    specificity_df.to_excel(writer, sheet_name='Specificity', index=False)\n",
    "\n",
    "    # Store classification report as text\n",
    "    report_df = pd.DataFrame(report.split('\\n'), columns=[\n",
    "                             'Classification Report'])\n",
    "    report_df.to_excel(writer, sheet_name='Classification Report', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
